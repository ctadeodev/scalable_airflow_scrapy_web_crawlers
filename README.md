# Scalable web crawling using Airflow and Scrapy
## Project overview

This project demonstrates a scalable web crawler that uses scrapy to crawl multiple websites. The scrapy crawlers will be orchestrated to run daily, weekly or monthly. Scraped data as will be stored in a PostgreSQL database. Crawler metadata and crawler high-water marks from previous run will also be saved on the PostgreSQL database.

---
## Tech stack
1. Scrapy for scraping
2. Airflow for orchestration
3. PostgreSQL for storing scrape json data and crawler metadata
4. Docker to containerize the project components
---
## Table of contents
[1. Container setup](#1-container-setup)  
[2. Database setup](#2-database-setup)  
[3. Scrapy web crawler setup](#3-scrapy-web-crawler-setup)  
[4. Airflow setup](#4-airflow-setup)

---
## 1. Container setup

---
## 2. Database setup

---
## 3. Scrapy web crawler setup

---
## 4. Airflow setup

---